{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af05649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import AdamW, SGD\n",
    "from testbed import ShortDataset, TextDataset, Trainer, Net0, Net1, Net2, Net3, Net4, Transformer\n",
    "from testbed.optim import Sonny\n",
    "from testbed.util import decode_broken_utf8, default_device, numel\n",
    "from testbed.gui import Plot, StatsTicker, ParameterInspector, Histogram, SmoothPlot, LinePlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e240cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"GPT2Transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162db02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_memory():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0) \n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    print(f\"Total {t}. Reserved {r}. Allocated {a}. Free {f}.\")\n",
    "    return (f, a, r, t) # code smell?\n",
    "\n",
    "def memory_allocated():\n",
    "    return torch.cuda.memory_allocated(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if network_name == \"Net0\":\n",
    "    num_input_classes= 256 # 256 possible UTF-8 bytes\n",
    "    embedding_dim = 32 # Dimension of embedding space. An embedding layer has 256 points in this space.\n",
    "    context_length = 32 # Number of sequential bytes visible to model (i.e. in the context)\n",
    "    num_hidden = 8192 # Hyperparameter for neural network\n",
    "    num_output_classes = 256 # 256 possible UTF-8 bytes\n",
    "    model = Net0(num_input_classes=num_input_classes,\n",
    "                 embedding_dim=embedding_dim,\n",
    "                 context_length=context_length,\n",
    "                 num_hidden=num_hidden,\n",
    "                 num_output_classes=num_output_classes,\n",
    "                 nonlinearity=\"sigmoid\").to(default_device())\n",
    "    example_length = context_length + 1\n",
    "    dataset = TextDataset(example_length=example_length)\n",
    "    batch_size = 512 # batch size (i.e. examples per batch)\n",
    "    OptimizerType = Sonny\n",
    "    optimizer_kwargs = {\"eps\": 1e-8, \"weight_decay\": 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc167e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if network_name == \"Net3\":\n",
    "    embedding_dim = 2\n",
    "    context_length = 32\n",
    "    num_hidden1 = 64\n",
    "    num_hidden2 = 64\n",
    "    model = Net3(embedding_dim=embedding_dim,\n",
    "                 context_length=context_length,\n",
    "                 num_hidden1=num_hidden1,\n",
    "                 num_hidden2=num_hidden2).to(default_device())\n",
    "    example_length = context_length + 1\n",
    "    dataset = TextDataset(example_length=example_length)\n",
    "    batch_size = 512\n",
    "    OptimizerType = Sonny\n",
    "    optimizer_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd38623",
   "metadata": {},
   "outputs": [],
   "source": [
    "if network_name == \"Net4\":\n",
    "    num_input_classes= 256 # 256 possible UTF-8 bytes\n",
    "    embedding_dim = 128 # Dimension of embedding space. An embedding layer has 256 points in this space.\n",
    "    context_length = 128 # Number of sequential bytes visible to model (i.e. in the context)\n",
    "    num_hidden = 4096 # Hyperparameter for neural network\n",
    "    num_output_classes = 256 # 256 possible UTF-8 bytes\n",
    "    model = Net4(num_input_classes=num_input_classes,\n",
    "                 embedding_dim=embedding_dim,\n",
    "                 context_length=context_length,\n",
    "                 num_hidden=num_hidden,\n",
    "                 num_output_classes=num_output_classes,\n",
    "                 nonlinearity=\"GELU\").to(default_device())\n",
    "    example_length = context_length + 1\n",
    "    dataset = TextDataset(example_length=example_length)\n",
    "    batch_size = 8192 # batch size (i.e. examples per batch)\n",
    "    OptimizerType = Sonny\n",
    "    optimizer_kwargs = {\"eps\": 1e-4, \n",
    "                        \"lr\": .0001, \n",
    "                        \"beta1\": .9, \n",
    "                        \"beta2\": .999,\n",
    "                        \"weight_decay\": 0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce28367",
   "metadata": {},
   "outputs": [],
   "source": [
    "if network_name == \"Transformer\":\n",
    "    model = Transformer(\n",
    "        n_vocab=256,\n",
    "        max_ctx=512,\n",
    "        d_model=1024,\n",
    "        n_heads=32,\n",
    "        d_ff=4096,\n",
    "        n_layers=4).to(default_device())\n",
    "    example_length = model.max_ctx + 1\n",
    "    dataset = TextDataset(example_length=example_length)\n",
    "    batch_size = 8 # batch size (i.e. examples per batch)\n",
    "    OptimizerType = Sonny\n",
    "    optimizer_kwargs = {\"eps\": 1e-6, \n",
    "                        \"lr\": .00001, \n",
    "                        \"beta1\": .9, \n",
    "                        \"beta2\": .999,\n",
    "                        \"weight_decay\": 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "if network_name == \"GPT2Transformer\":\n",
    "    ModelType = Transformer\n",
    "    model_kwargs = {\n",
    "        \"n_vocab\":50257,\n",
    "        \"max_ctx\":512,\n",
    "        \"d_model\":768,\n",
    "        \"n_heads\":12,\n",
    "        \"d_ff\":3072,\n",
    "        \"n_layers\":12}\n",
    "    model = ModelType(**model_kwargs).to(default_device())\n",
    "    example_length = 128 + 1 # like bert-uncased\n",
    "    dataset = ShortDataset(example_length=example_length)\n",
    "    batch_size = 256 # batch size (i.e. examples per batch)\n",
    "    OptimizerType = Sonny\n",
    "    optimizer_kwargs = {\"eps\": 1e-4, \n",
    "                        \"lr\": .0001, \n",
    "                        \"beta1\": .9, \n",
    "                        \"beta2\": .999,\n",
    "                        \"weight_decay\": 0.01,\n",
    "                        \"warmup\": 10000}\n",
    "    DatasetType = ShortDataset\n",
    "    dataset_kwargs = {\"example_length\": example_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, \n",
    "                  example_length=example_length, \n",
    "                  batch_size=batch_size,\n",
    "                  OptimizerType=OptimizerType,\n",
    "                  optimizer_kwargs=optimizer_kwargs,\n",
    "                  DatasetType=DatasetType,\n",
    "                  dataset_kwargs=dataset_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numel(model), model.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = StatsTicker(trainer, kind='line')\n",
    "ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsTicker(trainer, x='compute_time', y='compute_energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5659db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, p) in model.named_parameters():\n",
    "    print(name, p.device, p.numel(), p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_optimizer_settings(lr=.00001, beta1=.9, beta2=.999, batch_size=256, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyles_constant = 9115131782/14818489608 * 8 # convert from gpt2 loss to bpc loss, estimated factor\n",
    "lyles_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.8/lyles_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a13f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyles_constant*trainer.losses[-1][\"mean_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a786884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "n_vocab = acmodel.n_vocab\n",
    "def gpt2autocomplete(model, prompt=\" A\", n_generate=128, max_ctx=None, device=None):\n",
    "    model_device = next(model.parameters()).device\n",
    "    if device is None:\n",
    "        device = model_device\n",
    "        inference_model = model\n",
    "    if device != model_device:\n",
    "        torch.save(model.state_dict(), \"autocomplete.pt\")\n",
    "        inference_model = ModelType(**model_kwargs)\n",
    "        inference_model.load_state_dict(torch.load(\"autocomplete.pt\",map_location='cpu'))\n",
    "        inference_model.eval() # since probs is marked no_grad, does this matter?\n",
    "    if max_ctx is None:\n",
    "        max_ctx = inference_model.max_ctx\n",
    "    prompt = tokenizer(prompt)['input_ids']\n",
    "    completion = prompt[:]\n",
    "    for idx in range(n_generate):\n",
    "        x = (torch.tensor(prompt, dtype=torch.long)\n",
    "                  .unsqueeze(0)\n",
    "                  .to('cpu')) # shape [1,L]\n",
    "        P = inference_model.probs(x).view(-1)[-n_vocab:]\n",
    "        prob_dist = torch.distributions.Categorical(P)\n",
    "        c_ord = prob_dist.sample().item()\n",
    "        prompt = prompt + [c_ord]\n",
    "        completion = completion + [c_ord]\n",
    "        if len(prompt) == max_ctx + 1:\n",
    "            prompt = prompt[1:]\n",
    "    return tokenizer.decode(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4333dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(gpt2autocomplete(prompt=\" As it happens, I know several such people.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(model.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118708c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.losses[-1][\"compute_time\"]/trainer.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.losses[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8364cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def floprecurse(d, s):\n",
    "    results = []\n",
    "    if type(d) == dict:\n",
    "        try:\n",
    "            if \"time\" in d and \"energy\" in d:\n",
    "                results.append(f'\"{s}\": {{\"time\": {d[\"time\"]}, \"energy\": {d[\"energy\"]},'\n",
    "                               f' \"tflops\"= {d[\"energy\"]/d[\"time\"]/1E12}}}')\n",
    "        except:\n",
    "            pass\n",
    "        for (k, v) in d.items():\n",
    "            results = results + floprecurse(v, f\"{s}.{k}\")\n",
    "    if type(d) == list:\n",
    "        for (k, v) in enumerate(d):\n",
    "            results = results + floprecurse(v, f\"{s}.{k}\")\n",
    "    return results            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9698691",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = trainer.losses[-1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a506ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = floprecurse(d, \"root\")\n",
    "for a in A:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e493ea",
   "metadata": {},
   "source": [
    "## `Net0` autocomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8989765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.autocomplete()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67bcaf5",
   "metadata": {},
   "source": [
    "### SmoothPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.array([[x['compute_time'],x['mean_loss']] for x in trainer.losses])\n",
    "X = L[:,0]\n",
    "Y = L[:,1]\n",
    "def smoother(data, lag):\n",
    "    cs = np.cumsum(data)\n",
    "    return (cs[lag:] - cs[:-lag])/lag\n",
    "\n",
    "class SmoothPlot(LinePlot):\n",
    "    def __init__(self, X=None, Y=None, lag=100, log=None):\n",
    "        if X is not None:\n",
    "            if Y is None:\n",
    "                Y = np.array(X)\n",
    "                X = np.array(range(len(X)))\n",
    "            else:\n",
    "                X = np.array(X)\n",
    "                Y = np.array(Y)\n",
    "            X = X[lag:]\n",
    "            Y = smoother(Y, lag)\n",
    "            if log:\n",
    "                X = np.log(X)/math.log(2)\n",
    "        super().__init__(X, Y)\n",
    "SmoothPlot(X, Y, lag=100, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2726057",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from testbed import ShortDataset\n",
    "dataset = ShortDataset(example_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca6f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "data = defaultdict(defaultdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231caea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"dog\"][\"cat\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d06c6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def timing_study():\n",
    "\n",
    "    n_vocab=50257\n",
    "    max_ctx=1024\n",
    "    d_model=1024\n",
    "    n_heads=32\n",
    "    d_ff=4096,\n",
    "    n_layers=4\n",
    "    n_ctx = 1024\n",
    "    d_head = d_model // n_heads\n",
    "    assert d_model == n_heads * d_head\n",
    "    torch.cuda.synchronize()\n",
    "    data = defaultdict(lambda: defaultdict(float))\n",
    "    X = torch.randn(n_ctx, d_model, device='cuda')\n",
    "    Q = X\n",
    "    K = X\n",
    "    V = X\n",
    "    input_shape = X.shape\n",
    "    def split_heads(x):\n",
    "        return x.view(x.shape[:-1] + (n_heads, d_head)).transpose(-2, -3).contiguous()\n",
    "    def merge_heads(x):\n",
    "        x = x.transpose(-2,-3).contiguous()\n",
    "        return x.view(x.shape[:-2] + (d_model,))\n",
    "    additive_mask = 1.0-1.0/torch.tril(torch.ones(n_ctx,n_ctx, device=X.device))\n",
    "    additive_mask = torch.zeros(n_ctx,n_ctx, device=X.device)\n",
    "    @torch.no_grad()\n",
    "    def compute(Q0,K0,V0,data, N):\n",
    "        torch.cuda.synchronize()\n",
    "        t = time()\n",
    "        for _ in range(N):\n",
    "            Q = split_heads(Q0)/math.sqrt(d_head)\n",
    "            K = split_heads(K0)\n",
    "            V = split_heads(V0)\n",
    "            torch.cuda.synchronize()\n",
    "            data[\"split_heads\"][\"data\"] += torch.numel(Q) + torch.numel(K) + torch.numel(V)\n",
    "        data[\"split_heads\"][\"energy\"] += 3.0 * N * (torch.numel(Q) + torch.numel(K) + torch.numel(V))\n",
    "        data[\"split_heads\"][\"time\"] += time() - t\n",
    "\n",
    "\n",
    "        t = time()\n",
    "        for _ in range(N):\n",
    "            QKT = torch.matmul(Q, K.transpose(-1,-2)).contiguous()\n",
    "            torch.cuda.synchronize()\n",
    "            data[\"matmul(Q,K^T)\"][\"data\"] += torch.numel(Q) + torch.numel(K) + torch.numel(QKT)\n",
    "        data[\"matmul(Q,K^T)\"][\"energy\"] += 3.0 * N * torch.numel(Q) * n_ctx\n",
    "        data[\"matmul(Q,K^T)\"][\"time\"] += time() - t\n",
    "\n",
    "        #print(f\"Q.shape={Q.shape}\")\n",
    "        #print(f\"K.shape={Q.shape}\")\n",
    "        #print(f\"QKT.shape={QKT.shape}\")\n",
    "        t = time()\n",
    "        for _ in range(N):\n",
    "            Z = QKT + additive_mask\n",
    "        torch.cuda.synchronize()\n",
    "        data[\"masking_attention\"][\"data\"] += N*(torch.numel(Z) + torch.numel(QKT) + torch.numel(additive_mask))\n",
    "        data[\"masking_attention\"][\"energy\"] += 3.0 * N * 4.0 * (torch.numel(Z)) # ?\n",
    "        data[\"masking_attention\"][\"time\"] += time() - t\n",
    "    \n",
    "\n",
    "#         t = time()\n",
    "#         for _ in range(N):\n",
    "#             (Zmax, _) = torch.max(Z,dim=-1,keepdim=True)\n",
    "#             Z = Z - Zmax # as in GPT-2\n",
    "#             EZ = torch.exp(Z)\n",
    "#             sumEZ = torch.sum(EZ,dim=-1,keepdim=True)\n",
    "#             A = EZ/sumEZ\n",
    "#             assert A.shape == EZ.shape\n",
    "#             torch.cuda.synchronize()\n",
    "#         data[\"compute_attention\"][\"energy\"] += 3.0 * N * 5.0 * (torch.numel(A)) # ?\n",
    "#         data[\"compute_attention\"][\"time\"] += time() - t\n",
    "        softmax = torch.nn.Softmax(dim=-1)\n",
    "        t = time()\n",
    "        for _ in range(N):\n",
    "            A = softmax(Z)\n",
    "            torch.cuda.synchronize()\n",
    "            data[\"compute_attention\"][\"data\"] += torch.numel(Z) + torch.numel(A)\n",
    "        data[\"compute_attention\"][\"energy\"] += 3.0 * N * 5.0 * (torch.numel(A)) # ?\n",
    "        data[\"compute_attention\"][\"time\"] += time() - t\n",
    "        \n",
    "        t = time()\n",
    "        for _ in range(N):\n",
    "            AV = torch.matmul(A,V)\n",
    "            torch.cuda.synchronize()\n",
    "            data[\"matmul(A,V)\"][\"data\"] += torch.numel(A) + torch.numel(V) + torch.numel(AV)\n",
    "        data[\"matmul(A,V)\"][\"energy\"] += 3.0 * N * (torch.numel(A) * d_head)\n",
    "        data[\"matmul(A,V)\"][\"time\"] += time() - t\n",
    "\n",
    "        t = time()\n",
    "        for _ in range(N):\n",
    "            mergedAV = merge_heads(AV)\n",
    "            torch.cuda.synchronize()\n",
    "            data[\"merge_heads\"][\"data\"] += torch.numel(AV)\n",
    "        data[\"merge_heads\"][\"energy\"] += 3.0 * N * (torch.numel(AV))\n",
    "        data[\"merge_heads\"][\"time\"] += time() - t\n",
    "    start_time = time()\n",
    "    idx = 0\n",
    "    while time() < start_time + 30:\n",
    "        compute(Q,K,V,data,64)\n",
    "        idx = idx + 1\n",
    "    return (idx, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, data = timing_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922744e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttime = sum(v[\"time\"] for (k,v) in data.items())\n",
    "tenergy = sum(v[\"energy\"] for (k,v) in data.items())\n",
    "\n",
    "for (k,v) in data.items():\n",
    "    print(k, v[\"time\"]/ttime, v[\"energy\"]/v[\"time\"]/1E12, v[\"data\"]/v[\"time\"]/1E9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenergy/ttime/1E12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b528b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.nn.Softmax(dim=-1)\n",
    "x = 1.0-1.0/torch.tril(torch.ones(5,5, device='cpu'))\n",
    "s(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea9e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
