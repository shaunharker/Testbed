{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12749494",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af05649",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import math\n",
    "from math import log, sin, cos, tan, exp, sqrt, pi\n",
    "import time\n",
    "from random import randrange\n",
    "import torch\n",
    "import numpy as np\n",
    "from classroom import Classroom\n",
    "from classroom import Student\n",
    "from classroom import BytesDataset\n",
    "from classroom import GutenbergBytesDataset\n",
    "from classroom import GutenbergBitsDataset\n",
    "from classroom import GutenbergGPT2Dataset\n",
    "from classroom import MLPLM, MyLM, ABPCNLM\n",
    "from classroom import TransformerLM\n",
    "from classroom import AdamW\n",
    "from classroom import Sonny\n",
    "from classroom import Floyd\n",
    "from classroom import Plot\n",
    "from classroom import Fun\n",
    "from classroom import Count\n",
    "from classroom import Sum\n",
    "from classroom import Diff\n",
    "from classroom import Log2Sum\n",
    "from classroom import KalmanFilter1D\n",
    "from classroom import MedianFilter\n",
    "from classroom import TwoWindowFilter\n",
    "from classroom import numel\n",
    "from classroom import utf8decode, utf8encode, gpt2decode, gpt2encode\n",
    "from classroom import utf8bitsdecode, utf8bitsencode\n",
    "from pathlib import Path\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa77de2",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    path = 'merged.pt' #\n",
    "    #path = '2021-11-27-0429.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d836c6c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = torch.load(path).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ddfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = (\n",
    "        MLPLM(\n",
    "            n_vocab_in=50257,\n",
    "            n_ctx=16,\n",
    "            d_model=16,\n",
    "            d_hidden=16,\n",
    "            nonlinearity=\"GELU\",\n",
    "            n_vocab_out=50257).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = (\n",
    "        MyLM(\n",
    "            n_vocab_in=50257,\n",
    "            n_ctx=65,\n",
    "            d_model=64,\n",
    "            n_layers=1,\n",
    "            d_hidden=256,\n",
    "            nonlinearity=\"GELU\",\n",
    "            p_dropout=0.00,\n",
    "            n_vocab_out=50257).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d536de8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = (\n",
    "        ABPCNLM(\n",
    "            n_vocab_in=50257,\n",
    "            n_ctx=128,\n",
    "            d_model=8,\n",
    "            n_layers=1,\n",
    "            d_hidden=1024,\n",
    "            nonlinearity=\"GELU\",\n",
    "            p_dropout=0.0,\n",
    "            n_vocab_out=50257).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4af084",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = (\n",
    "        TransformerLM(\n",
    "            n_vocab_in=50257,\n",
    "            n_vocab_out=50257,\n",
    "            n_ctx=1024,\n",
    "            d_model=2**12,\n",
    "            d_k=2**6,\n",
    "            d_v=2**6,\n",
    "            n_heads=2**6,\n",
    "            d_hidden=2**13,\n",
    "            n_layers=8,\n",
    "            p_dropout_embedding=0,\n",
    "            p_dropout_attn_mat=0,\n",
    "            p_dropout_attn_out=0,\n",
    "            p_dropout_mlp=0).to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4965c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "numel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed9ec4",
   "metadata": {},
   "source": [
    "## initialize student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44e13f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(parameters=model.named_parameters())\n",
    "dataset = GutenbergGPT2Dataset(device='cpu')\n",
    "#dataset = GutenbergBytesDataset()\n",
    "\n",
    "batch_size = None\n",
    "example_length = model.n_ctx + 1\n",
    "\n",
    "student= Student(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    example_length=example_length,\n",
    "    device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d10566",
   "metadata": {},
   "source": [
    "## schedule hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25468e2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "student.batch_size = 1\n",
    "student.example_length = 1025\n",
    "for (idx, (pn, p)) in enumerate(student.model.named_parameters()):\n",
    "    batch_multiplier = 1\n",
    "    lr_base = 1e-6\n",
    "    warm_up = 0\n",
    "    lr = lambda n: 0 if n < warm_up else lr_base *(n%10)/10 \n",
    "    student.optimizer.state[pn][\"lr\"]           = lambda n: lr(n)\n",
    "    student.optimizer.state[pn][\"beta1\"]        = lambda n: 0.9\n",
    "    student.optimizer.state[pn][\"beta2\"]        = lambda n: 0.999\n",
    "    student.optimizer.state[pn][\"weight_decay\"] = lambda n: 0.001\n",
    "    student.optimizer.state[pn][\"update\"]       = lambda n: (n < warm_up) or (n%batch_multiplier == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23bd3c",
   "metadata": {},
   "source": [
    "## test a single iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.language_model.crossentropyloss.crossentropyloss = torch.nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5161d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.study()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c6bd0",
   "metadata": {},
   "source": [
    "## initialize baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ae8b7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    student.reset_baseline()\n",
    "    n_of_last_baseline = len(student.times)\n",
    "    t_start = time.time()\n",
    "    t_of_last_baseline = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25202dea",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def train(student):\n",
    "    while True:\n",
    "        student.study()\n",
    "        await asyncio.sleep(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371cb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_task = asyncio.create_task(train(student))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef798d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33459961",
   "metadata": {},
   "source": [
    "## autocomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88519c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete(model, prompt=None, n_generate=512,\n",
    "                     n_ctx=None, temp=1.0,\n",
    "                     encode=None, decode=None, output=None):\n",
    "    Categorical = torch.distributions.Categorical\n",
    "    if n_ctx is None:\n",
    "        n_ctx = model.n_ctx\n",
    "    if encode is None:\n",
    "        encode = gpt2encode\n",
    "        #encode = utf8encode\n",
    "    if decode is None:\n",
    "        decode = gpt2decode\n",
    "        #decode = utf8decode\n",
    "    if prompt is None:\n",
    "        prompt = decode(student.dataset.batch(1, 2*n_ctx, offset=None).tolist()[0])  # kludge\n",
    "    x = encode(prompt)\n",
    "    x = x[-n_ctx:]\n",
    "    prompt = decode(x)\n",
    "    print(f\"=== Prompt ===\\n{prompt}\\n=== Autocompletion ===\\n\")\n",
    "\n",
    "    def sampler(x):\n",
    "        x = list(x)\n",
    "        for _ in range(n_generate):\n",
    "            probs = model.inference(torch.tensor(x, dtype=torch.long, device=\"cuda\").unsqueeze(0)).view(-1)[-model.n_vocab_out:]\n",
    "            if temp > 0:\n",
    "                y = Categorical(probs=probs**(1.0/temp)).sample().item()\n",
    "            else:\n",
    "                y = torch.argmax(probs).item()\n",
    "            x = (x + [y])[-n_ctx:]\n",
    "            if output is not None:\n",
    "                output.append(y)\n",
    "            yield y\n",
    "    result = decode(list(sampler(x)))\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491d8cc",
   "metadata": {},
   "source": [
    "When first the opposition of fact and ideal grows fully visible, a\n",
    "spirit of fiery revolt, of fierce hatred of the gods, seems necessary\n",
    "to the assertion of freedom. To defy with Promethean constancy a hostile\n",
    "universe, to keep its evil always in view, always actively hated, to\n",
    "refuse no pain that the malice of Power can invent, appears to be the\n",
    "duty of all who will not bow before the inevitable. But indignation is\n",
    "still a bondage, for it compels our thoughts to be occupied with an evil\n",
    "world; and in the fierceness of desire from which rebellion springs there\n",
    "is a kind of self-assertion which it is necessary for the wise to overcome.\n",
    "Indignation is a submission of our thoughts, but not of our desires; the\n",
    "Stoic freedom in which wisdom consists is found in the submission of our\n",
    "desires, but not of our thoughts. From the submission of our desires springs\n",
    "the virtue of resignation; from the freedom of our thoughts springs the whole\n",
    "world of art and philosophy, and the vision of beauty by which, at last, we\n",
    "half reconquer the reluctant world.\n",
    "\n",
    "When first the opposition of fact and ideal grows fully visible, a spirit\n",
    "of fiery revolt, of fierce hatred of the gods, seems necessary to the\n",
    "assertion of freedom. To defy with Promethean constancy a hostile universe,\n",
    "to keep its evil always in view, always actively hated, to refuse no pain\n",
    "that the malice of Power can invent, appears to be the duty of all who will\n",
    "not bow before the inevitable. But indignation is still a bondage, for it\n",
    "compels our thoughts to be occupied with an evil world; and in the fierceness\n",
    "of desire from which rebellion springs there is a kind of self-assertion\n",
    "which it is necessary for the wise to overcome. Indignation is a submission\n",
    "of our thoughts, but not of our desires; the Stoic freedom in which wisdom\n",
    "consists is found in the submission of our desires, but not of our thoughts.\n",
    "From the submission of our desires springs the virtue of resignation; from\n",
    "the freedom of our thoughts springs the whole world of art and philosophy,\n",
    "and the vision of beauty by which, at last, we half reconquer the reluctant\n",
    "world.\n",
    "...\n",
    "\n",
    "We are of the sun and the moon and the stars.\n",
    "The power manifested in the mind of God is projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad26652",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "autocomplete(model=student.model, temp=1.0, n_ctx=128, n_generate=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67bcaf5",
   "metadata": {},
   "source": [
    "## plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b45bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "plot_data = {}\n",
    "lag = 4096*4\n",
    "X = Fun(Log2Sum(), student.times)\n",
    "Y = Fun(TwoWindowFilter(lag=lag), student.grades)\n",
    "Z = Fun(TwoWindowFilter(lag=lag), student.baseline_grades)\n",
    "plot_data.update({f\"grades\": (X, Y)})\n",
    "plot_data.update({f\"baseline\": (X, Z)})\n",
    "Plot(**plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86748c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(student.grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab270a92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "plot_data_2 = {}\n",
    "lag = 10\n",
    "X = Fun(Count(), student.times)\n",
    "Y = Fun(lambda x, y: x - y, student.grades, student.baseline_grades)\n",
    "Y = Fun(TwoWindowFilter(lag=lag), Y.output, aux=Y)\n",
    "plot_data_2.update({f\"improvement\": (X, Y)})\n",
    "Plot(**plot_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c32f3b",
   "metadata": {},
   "source": [
    "## stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n = len(student.times)-1\n",
    "t = time.time() - t_start\n",
    "dn = n - n_of_last_baseline\n",
    "dt = t - t_of_last_baseline\n",
    "\n",
    "N = max(1, dn)\n",
    "y = np.mean(np.array(student.grades[n-N:n]))\n",
    "y0 = np.mean(np.array(student.baseline_grades[n-N:n]))\n",
    "dy = (y - y0) # assuming linear growth\n",
    "z = y0 + dy\n",
    "\n",
    "message = '\\n'.join([\n",
    "    f\"batch_size            = {student.batch_size}\",\n",
    "    f\"example_length        = {student.example_length}\",\n",
    "    f\"100*z                 = {int(z*1e6)/1e4}\",\n",
    "    f\"n                     = {n} steps\",\n",
    "    f\"t                     = {int(t)} seconds\",\n",
    "    f\"n_of_last_baseline    = {n_of_last_baseline} steps\",\n",
    "    f\"t_of_last_baseline    = {int(t_of_last_baseline)} seconds\",\n",
    "    f\"steps per second      = {dn/dt}\",\n",
    "    f\"y0                    = {int(y0*1e6)/1e6}\",\n",
    "    f\"dy                    = {int(dy*1e6)/1e6}\",\n",
    "    f\"dn                    = {dn}\",\n",
    "    f\"dt                    = {dt}\",\n",
    "    f\"dy/dn                 = {dy/dn}\",\n",
    "    f\"dy/dt                 = {int(1e9 * dy/dt)}e-9 per second\",\n",
    "    f\"time for 100%         = {(1.0)/(dy/dt)//36/100} hours\",\n",
    "])\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    student.reset_baseline()\n",
    "    n_of_last_baseline = len(student.times)-1\n",
    "    t_of_last_baseline = time.time() - t_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe9998",
   "metadata": {},
   "source": [
    "## save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c88c06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.save(student.model, f=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.example_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def autosave():\n",
    "    while True:\n",
    "        await asyncio.sleep(3600)\n",
    "        torch.save(student.model, f='autosave.pt')\n",
    "task = asyncio.create_task(autosave())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_per_token = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e05f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lyles_constant = (9115131782/2)/14818489608 * log(50257)/log(256)\n",
    "lyles_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfeaeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf8grade = lambda x: 1 - (1 - x)*lyles_constant\n",
    "grade = .7343\n",
    "print(f\"gpt2 grade = {grade}, utf8 grade = {utf8grade(grade)}, bpc = {(1-utf8grade(grade))*8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0bd9d9",
   "metadata": {},
   "source": [
    "## parameter histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6da8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histograms = []\n",
    "for (idx, (pn, p)) in enumerate(student.model.named_parameters()):\n",
    "    with torch.no_grad():\n",
    "        print(idx, pn, torch.sqrt(torch.var(p)).item())\n",
    "        Y, X = np.histogram(p.detach().cpu().numpy(), bins=int(sqrt(torch.numel(p))), density=True)\n",
    "        print(X.shape, Y.shape)\n",
    "        histograms.append(Plot(**{f\"hist-{idx}\": (X.tolist(), Y.tolist())}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92345fea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histograms[4] # 3 7 9 13 15 21 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd932e",
   "metadata": {},
   "source": [
    "## batch-level grade histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149c525",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y, X = np.histogram(student.grades[-5000:], bins=256, range=(0,1.0), density=True)\n",
    "V, U = np.histogram(student.baseline_grades[-5000:], bins=256, range=(0,1.0), density=True)\n",
    "Plot(**{f\"grade-hist\": (X, Y), \"baseline\": (U, V)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e52587",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.n_ctx, model.d_model, model.d_hidden, model.n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e8670",
   "metadata": {},
   "source": [
    "## example-level grade histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a6829",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_graded_examples():\n",
    "    result = []\n",
    "    for batch_idx in range(16):\n",
    "        print(f\"batch_idx = {batch_idx}/256\")\n",
    "        x = student.dataset.batch(student.batch_size, student.example_length)\n",
    "        print(f\"orig {x.shape}\")\n",
    "        with torch.no_grad():\n",
    "            y = student.model(x)\n",
    "            y = 1.0 - y.cpu().numpy()\n",
    "            result.append(y)\n",
    "    data = np.concatenate(result, axis=0)\n",
    "    result = data.tolist()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69f1e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graded_examples = get_graded_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41830fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graded_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b19424",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum(x for x in graded_examples)/len(graded_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9715135b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "R = (0, 1)\n",
    "def XYFor(k):\n",
    "    es = graded_examples\n",
    "    bins = int(sqrt(len(es)))\n",
    "    Y, X = np.histogram(es, bins=bins, range=R, density=True)\n",
    "    return (X, Y)\n",
    "Plot(**{f\"examples-hist-{k}\": XYFor(k) for k in [1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdc907",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ord(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04797a40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(example_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cffc0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(1 - 0.7870894884999871)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7599c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(1 - 0.8)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f2127",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(1 - 0.9)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45748779",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.array([[1,2],[3,4]],dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ccd89a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.unpackbits(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea23ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
