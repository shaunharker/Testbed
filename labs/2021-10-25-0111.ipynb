{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12749494",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7093a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sharker/github/scholar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6af05649",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import math\n",
    "from math import log, sin, cos, tan, exp, sqrt, pi\n",
    "import time\n",
    "from random import randrange\n",
    "import torch\n",
    "import numpy as np\n",
    "from scholar.trainer import Trainer\n",
    "from scholar.dataset import GutenbergGPT2Dataset\n",
    "from scholar.model import TransformerLM\n",
    "from scholar.optimizer import AdamW\n",
    "from scholar.autocomplete import autocomplete\n",
    "from scholar import numel\n",
    "import scholar as classroom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa77de2",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a0b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    path = '2021-10-25-0111.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d836c6c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'classroom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37703/4247306614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'classroom'"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    model = torch.load(path).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ddfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = (\n",
    "        MLPLM(\n",
    "            n_vocab_in=50257,\n",
    "            n_ctx=16,\n",
    "            d_model=16,\n",
    "            d_hidden=16,\n",
    "            nonlinearity=\"GELU\",\n",
    "            n_vocab_out=50257).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = (\n",
    "        MyLM(\n",
    "            n_vocab_in=50257,\n",
    "            n_ctx=65,\n",
    "            d_model=64,\n",
    "            n_layers=1,\n",
    "            d_hidden=256,\n",
    "            nonlinearity=\"GELU\",\n",
    "            p_dropout=0.00,\n",
    "            n_vocab_out=50257).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d536de8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = (\n",
    "        ABPCNLM(\n",
    "            n_vocab_in=50257,\n",
    "            n_ctx=128,\n",
    "            d_model=8,\n",
    "            n_layers=1,\n",
    "            d_hidden=1024,\n",
    "            nonlinearity=\"GELU\",\n",
    "            p_dropout=0.0,\n",
    "            n_vocab_out=50257).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4af084",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = (\n",
    "        TransformerLM(\n",
    "            n_vocab_in=50257,\n",
    "            n_vocab_out=50257,\n",
    "            n_ctx=1024,\n",
    "            d_model=1024,\n",
    "            d_k=32,\n",
    "            d_v=32,\n",
    "            n_heads=32,\n",
    "            d_hidden=4096,\n",
    "            n_layers=3,\n",
    "            p_dropout_embedding=0,\n",
    "            p_dropout_attn_mat=0,\n",
    "            p_dropout_attn_out=0,\n",
    "            p_dropout_mlp=0).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4965c860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141810769"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed9ec4",
   "metadata": {},
   "source": [
    "## initialize student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d44e13f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(parameters=model.named_parameters())\n",
    "dataset = GutenbergGPT2Dataset()\n",
    "batch_size = None\n",
    "example_length = model.n_ctx + 1\n",
    "\n",
    "student= Student(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    example_length=example_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d10566",
   "metadata": {},
   "source": [
    "## schedule hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25468e2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "student.batch_size = 1\n",
    "student.example_length = 1025\n",
    "for (idx, (pn, p)) in enumerate(student.model.named_parameters()):\n",
    "    batch_multiplier = 100\n",
    "    lr_base = 1e-8\n",
    "    warm_up = 1000\n",
    "    lr = lambda n: 0 if n < warm_up else lr_base *(n%1000)/1000 \n",
    "    student.optimizer.state[pn][\"lr\"]           = lambda n: lr(n)\n",
    "    student.optimizer.state[pn][\"beta1\"]        = lambda n: 0.9\n",
    "    student.optimizer.state[pn][\"beta2\"]        = lambda n: 0.999\n",
    "    student.optimizer.state[pn][\"weight_decay\"] = lambda n: 0.001\n",
    "    student.optimizer.state[pn][\"update\"]       = lambda n: (n < warm_up) or (n%batch_multiplier == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23bd3c",
   "metadata": {},
   "source": [
    "## test a single iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.language_model.crossentropyloss.crossentropyloss = torch.nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5161d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student.study()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c6bd0",
   "metadata": {},
   "source": [
    "## initialize baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ae8b7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    student.reset_baseline()\n",
    "    n_of_last_baseline = len(student.times)\n",
    "    t_start = time.time()\n",
    "    t_of_last_baseline = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25202dea",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def train(student):\n",
    "    while True:\n",
    "        student.study()\n",
    "        await asyncio.sleep(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371cb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_task = asyncio.create_task(train(student))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef798d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33459961",
   "metadata": {},
   "source": [
    "## autocomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f88519c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete(model, prompt=None, n_generate=512,\n",
    "                     n_ctx=None, temp=1.0,\n",
    "                     encode=None, decode=None, output=None):\n",
    "    Categorical = torch.distributions.Categorical\n",
    "    if n_ctx is None:\n",
    "        n_ctx = model.n_ctx\n",
    "    if encode is None:\n",
    "        encode = gpt2encode\n",
    "    if decode is None:\n",
    "        decode = gpt2decode\n",
    "    if prompt is None:\n",
    "        prompt = decode(student.dataset.batch(1, 2*n_ctx, offset=None).tolist()[0])  # kludge\n",
    "    x = encode(prompt)\n",
    "    x = x[-n_ctx:]\n",
    "    prompt = decode(x)\n",
    "    print(f\"=== Prompt ===\\n{prompt}\\n=== Autocompletion ===\\n\")\n",
    "\n",
    "    def sampler(x):\n",
    "        x = list(x)\n",
    "        for _ in range(n_generate):\n",
    "            probs = model.inference(torch.tensor(x, dtype=torch.long, device=\"cuda\").unsqueeze(0)).view(-1)[-model.n_vocab_out:]\n",
    "            if temp > 0:\n",
    "                y = Categorical(probs=probs**(1.0/temp)).sample().item()\n",
    "            else:\n",
    "                y = torch.argmax(probs).item()\n",
    "            x = (x + [y])[-n_ctx:]\n",
    "            if output is not None:\n",
    "                output.append(y)\n",
    "            yield y\n",
    "    result = decode(list(sampler(x)))\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491d8cc",
   "metadata": {},
   "source": [
    "When first the opposition of fact and ideal grows fully visible, a\n",
    "spirit of fiery revolt, of fierce hatred of the gods, seems necessary\n",
    "to the assertion of freedom. To defy with Promethean constancy a hostile\n",
    "universe, to keep its evil always in view, always actively hated, to\n",
    "refuse no pain that the malice of Power can invent, appears to be the\n",
    "duty of all who will not bow before the inevitable. But indignation is\n",
    "still a bondage, for it compels our thoughts to be occupied with an evil\n",
    "world; and in the fierceness of desire from which rebellion springs there\n",
    "is a kind of self-assertion which it is necessary for the wise to overcome.\n",
    "Indignation is a submission of our thoughts, but not of our desires; the\n",
    "Stoic freedom in which wisdom consists is found in the submission of our\n",
    "desires, but not of our thoughts. From the submission of our desires springs\n",
    "the virtue of resignation; from the freedom of our thoughts springs the whole\n",
    "world of art and philosophy, and the vision of beauty by which, at last, we\n",
    "half reconquer the reluctant world.\n",
    "\n",
    "When first the opposition of fact and ideal grows fully visible, a spirit\n",
    "of fiery revolt, of fierce hatred of the gods, seems necessary to the\n",
    "assertion of freedom. To defy with Promethean constancy a hostile universe,\n",
    "to keep its evil always in view, always actively hated, to refuse no pain\n",
    "that the malice of Power can invent, appears to be the duty of all who will\n",
    "not bow before the inevitable. But indignation is still a bondage, for it\n",
    "compels our thoughts to be occupied with an evil world; and in the fierceness\n",
    "of desire from which rebellion springs there is a kind of self-assertion\n",
    "which it is necessary for the wise to overcome. Indignation is a submission\n",
    "of our thoughts, but not of our desires; the Stoic freedom in which wisdom\n",
    "consists is found in the submission of our desires, but not of our thoughts.\n",
    "From the submission of our desires springs the virtue of resignation; from\n",
    "the freedom of our thoughts springs the whole world of art and philosophy,\n",
    "and the vision of beauty by which, at last, we half reconquer the reluctant\n",
    "world.\n",
    "...\n",
    "\n",
    "We are of the sun and the moon and the stars.\n",
    "The power manifested in the mind of God is projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad26652",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt ===\n",
      "\n",
      "When first the opposition of fact and ideal grows fully visible, a spirit\n",
      "of fiery revolt, of fierce hatred of the gods, seems necessary to the\n",
      "assertion of freedom. To defy with Promethean constancy a hostile universe,\n",
      "to keep its evil always in view, always actively hated, to refuse no pain\n",
      "that the malice of Power can invent, appears to be the duty of all who will\n",
      "not bow before the inevitable. But indignation is still a bondage, for it\n",
      "compels our thoughts to be occupied with an evil world; and in the fierceness\n",
      "of desire from which rebellion springs there is a kind of self-assertion\n",
      "which it is necessary for the wise to overcome. Indignation is a submission\n",
      "of our thoughts, but not of our desires; the Stoic freedom in which wisdom\n",
      "consists is found in the submission of our desires, but not of our thoughts.\n",
      "From the submission of our desires springs the virtue of resignation; from\n",
      "the freedom of our thoughts springs the whole world of art and philosophy,\n",
      "and the vision of beauty by which, at last, we half reconquer the reluctant\n",
      "world.\n",
      "\n",
      "=== Autocompletion ===\n",
      "\n",
      "\n",
      "When we have undertaken to fulfil our present purpose to the present time,\n",
      "that postponed till the last stages of discussion, and followed it, we\n",
      "shall proceed; and at least, what facts the choice on earth may have\n",
      "been brought into prominence with impartiality. Compare the position of\n",
      "kills; we shall prolong our knowledge, and follow the direction of\n",
      "analogous logic, foreseeing up as long as we may unification course of\n",
      "history. When our plunderers did wrong, we once began to regard it as a\n",
      "subordinating relic, as a whole, to this secret passage, the\n",
      "vast resources of our instrument could ensue; that we regarded with a\n",
      "view we might be permitted to examine the rights of others. In our\n",
      "chapters nearly reached the conclusion that the gods we omitted to the\n",
      "honour of the gods; we, therefore, do we now devote the lives of the\n",
      "latter.\n",
      "\n",
      "The criticism of religion afforded us more favourable neighbours to the\n",
      "religious antipope; and the consideration of affairs at this moment,\n",
      "which possess a profound and lasting melancholy. The beggar replied\n",
      "to these defects; that to have a certain fascination would peremptue\n",
      "his heart and figure, and remake the essence of these sentiments,\n",
      "scattering arms, flatterers, knight, or make any ill-humour. But now,\n",
      "at least, our passage is opposed to pursuing men delineating them with\n",
      "the actual object of charity, and giving their opinion of their desire\n",
      "as they cheat the unregenerate husbands, and their helplessness of virtue\n",
      "than could render the one sinner of another. The poor merely wrote,\n",
      "staring strangers, and flourishing themselves like vulgar flowers, enacting\n",
      "with what they had after sponsors, but apparently incapable of matching\n",
      "their own opinions,--though, particularly, the destructive essence of\n",
      "Paolo seems singularly the expression of our conscious religion; but the\n",
      "infidelity and hypocrisy which burns all things afterwards, in the\n",
      "former channels, become aggressive and hybrid, till reason reaches the\n",
      "innover's brink to the right end of our creed; maintaining it with all\n",
      "imperceptible sublimities, which we once look upon, and see what we feel\n",
      "its effects upon it, present us to this view of\n",
      "CPU times: user 14.4 s, sys: 0 ns, total: 14.4 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "autocomplete(model=student.model, prompt=\"\"\"\n",
    "When first the opposition of fact and ideal grows fully visible, a spirit\n",
    "of fiery revolt, of fierce hatred of the gods, seems necessary to the\n",
    "assertion of freedom. To defy with Promethean constancy a hostile universe,\n",
    "to keep its evil always in view, always actively hated, to refuse no pain\n",
    "that the malice of Power can invent, appears to be the duty of all who will\n",
    "not bow before the inevitable. But indignation is still a bondage, for it\n",
    "compels our thoughts to be occupied with an evil world; and in the fierceness\n",
    "of desire from which rebellion springs there is a kind of self-assertion\n",
    "which it is necessary for the wise to overcome. Indignation is a submission\n",
    "of our thoughts, but not of our desires; the Stoic freedom in which wisdom\n",
    "consists is found in the submission of our desires, but not of our thoughts.\n",
    "From the submission of our desires springs the virtue of resignation; from\n",
    "the freedom of our thoughts springs the whole world of art and philosophy,\n",
    "and the vision of beauty by which, at last, we half reconquer the reluctant\n",
    "world.\n",
    "\"\"\", temp=1.0, n_ctx=1024, n_generate=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67bcaf5",
   "metadata": {},
   "source": [
    "## plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b45bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "plot_data = {}\n",
    "lag = 4096*4\n",
    "X = Fun(Log2Sum(), student.times)\n",
    "Y = Fun(TwoWindowFilter(lag=lag), student.grades)\n",
    "Z = Fun(TwoWindowFilter(lag=lag), student.baseline_grades)\n",
    "plot_data.update({f\"grades\": (X, Y)})\n",
    "plot_data.update({f\"baseline\": (X, Z)})\n",
    "Plot(**plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86748c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab270a92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "plot_data_2 = {}\n",
    "lag = 1000\n",
    "X = Fun(Count(), student.times)\n",
    "Y = Fun(lambda x, y: x - y, student.grades, student.baseline_grades)\n",
    "Y = Fun(TwoWindowFilter(lag=lag), Y.output, aux=Y)\n",
    "plot_data_2.update({f\"improvement\": (X, Y)})\n",
    "Plot(**plot_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c32f3b",
   "metadata": {},
   "source": [
    "## stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n = len(student.times)-1\n",
    "t = time.time() - t_start\n",
    "dn = n - n_of_last_baseline\n",
    "dt = t - t_of_last_baseline\n",
    "\n",
    "N = min(10000, (dn//2000)*1000)\n",
    "if N == 0:\n",
    "    N = dn // 2\n",
    "y = np.mean(np.array(student.grades[n-N:n]))\n",
    "y0 = np.mean(np.array(student.baseline_grades[n-N:n]))\n",
    "dy = y - y0\n",
    "\n",
    "\n",
    "message = '\\n'.join([\n",
    "    f\"batch_size            = {student.batch_size}\",\n",
    "    f\"example_length        = {student.example_length}\",\n",
    "    f\"100*y                 = {int(y*1e6)/1e4}\",\n",
    "    f\"n                     = {n} steps\",\n",
    "    f\"t                     = {int(t)} seconds\",\n",
    "    f\"n_of_last_baseline    = {n_of_last_baseline} steps\",\n",
    "    f\"t_of_last_baseline    = {int(t_of_last_baseline)} seconds\",\n",
    "    f\"steps per second      = {dn/dt}\",\n",
    "    f\"y0                    = {int(y0*1e6)/1e6}\",\n",
    "    f\"dy                    = {int(dy*1e9)}e-09\",\n",
    "    f\"dn                    = {dn}\",\n",
    "    f\"dt                    = {dt}\",\n",
    "    f\"dy/dn                 = {dy/dn}\",\n",
    "    f\"dy/dt                 = {int(1e12 * dy/dt)}e-12 per second\",\n",
    "    f\"time for .1%          = {(.01)/(dy/dt)//36/1000} hours\",\n",
    "])\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    student.reset_baseline()\n",
    "    n_of_last_baseline = len(student.times)-1\n",
    "    t_of_last_baseline = time.time() - t_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe9998",
   "metadata": {},
   "source": [
    "## save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c88c06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.save(student.model, f=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def autosave():\n",
    "    while True:\n",
    "        await asyncio.sleep(3600)\n",
    "        torch.save(student.model, f='autosave.pt')\n",
    "task = asyncio.create_task(autosave())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_per_token = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e05f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lyles_constant = (9115131782/2)/14818489608 * log(50257)/log(256)\n",
    "lyles_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfeaeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf8grade = lambda x: 1 - (1 - x)*lyles_constant\n",
    "grade = .7343\n",
    "print(f\"gpt2 grade = {grade}, utf8 grade = {utf8grade(grade)}, bpc = {(1-utf8grade(grade))*8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0bd9d9",
   "metadata": {},
   "source": [
    "## parameter histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6da8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histograms = []\n",
    "for (idx, (pn, p)) in enumerate(student.model.named_parameters()):\n",
    "    with torch.no_grad():\n",
    "        print(idx, pn, torch.sqrt(torch.var(p)).item())\n",
    "        Y, X = np.histogram(p.detach().cpu().numpy(), bins=int(sqrt(torch.numel(p))), density=True)\n",
    "        print(X.shape, Y.shape)\n",
    "        histograms.append(Plot(**{f\"hist-{idx}\": (X.tolist(), Y.tolist())}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92345fea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histograms[4] # 3 7 9 13 15 21 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd932e",
   "metadata": {},
   "source": [
    "## batch-level grade histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149c525",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y, X = np.histogram(student.grades[-5000:], bins=256, range=(0,1.0), density=True)\n",
    "V, U = np.histogram(student.baseline_grades[-5000:], bins=256, range=(0,1.0), density=True)\n",
    "Plot(**{f\"grade-hist\": (X, Y), \"baseline\": (U, V)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e52587",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.n_ctx, model.d_model, model.d_hidden, model.n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e8670",
   "metadata": {},
   "source": [
    "## example-level grade histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a6829",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_graded_examples():\n",
    "    result = []\n",
    "    for batch_idx in range(16):\n",
    "        print(f\"batch_idx = {batch_idx}/256\")\n",
    "        x = student.dataset.batch(student.batch_size, student.example_length)\n",
    "        print(f\"orig {x.shape}\")\n",
    "        with torch.no_grad():\n",
    "            y = student.model(x)\n",
    "            y = 1.0 - y.cpu().numpy()\n",
    "            result.append(y)\n",
    "    data = np.concatenate(result, axis=0)\n",
    "    result = data.tolist()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69f1e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graded_examples = get_graded_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41830fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graded_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b19424",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum(x for x in graded_examples)/len(graded_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9715135b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "R = (0, 1)\n",
    "def XYFor(k):\n",
    "    es = graded_examples\n",
    "    bins = int(sqrt(len(es)))\n",
    "    Y, X = np.histogram(es, bins=bins, range=R, density=True)\n",
    "    return (X, Y)\n",
    "Plot(**{f\"examples-hist-{k}\": XYFor(k) for k in [1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdc907",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ord(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04797a40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(example_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cffc0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(1 - 0.7870894884999871)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7599c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(1 - 0.8)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f2127",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(1 - 0.9)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45748779",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.array([[1,2],[3,4]],dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ccd89a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.unpackbits(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea23ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
